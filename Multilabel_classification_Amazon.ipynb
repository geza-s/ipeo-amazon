{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planet : Amazon space -- Full model running for multilabel classification\n",
    "This file is the main processing file for the multilabel classification by a single CNN model.\n",
    "Important information:\n",
    "- The part 0 to 4 are necessary for the training and the testing.\n",
    "- The training part can be skipped to simply test the model. Don't forget to load a trained model if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Initialization\n",
    "## 0.1) Getting Module and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Multilabel_Amazon_Engine import *\n",
    "from Multilabel_Amazon_Module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_folder = '../IPEO_Planet_project'\n",
    "if not os.path.exists(data_folder):\n",
    "    data_folder = input(\"Enter the data folder path: \")\n",
    "    assert os.path.exists(data_folder), \"I did not find the folder at, \"+str(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Putting the model on the gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = MultiLayerCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(24,3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Getting the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_csv = 'training.csv'\n",
    "validation_csv = 'validation.csv'\n",
    "test_csv = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Transform_choice = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = AmazonSpaces(csv_file=train_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "validation_dataset = AmazonSpaces(csv_file=validation_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "test_dataset = AmazonSpaces(csv_file=test_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Wrapping into the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "# Basic training, can be slow....\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "# With num worker = 6, for quicker training....\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=6, shuffle=True, drop_last = True)\n",
    "#validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=6, shuffle=True, drop_last = True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Choice of Criterion and Opitimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A) Choice of loss function (Optional weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion_with_weights = False # True/False if weights to be set to optimiser\n",
    "if criterion_with_weights:\n",
    "    weights = torch.load('class_weight.pt')\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights.to(device))\n",
    "    print(f\"Weighted optimizer set to : \\n {np.around(weights.detach().numpy(), decimals=3)}\")\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B) Choice of optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs_number = 2\n",
    "\n",
    "Train_results = train(model, train_dataloader, validation_dataloader, device=device, optimizer= optim, lr = learning_rate, epochs=epochs_number, loss_fn=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.B) Saving the results into a Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create json object from dictionary\n",
    "js = json.dumps(Train_results)\n",
    "\n",
    "# open file for writing, \"w\"\n",
    "name = f\"MultilabelModel_train_results_{epochs_number}epochs_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{criterion}_{str(optim).split()[0]}_optim_{str(optim).split()[16].replace('.','_')}_mom_{date.today()}.json\"\n",
    "f = open(name,\"a\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(js)\n",
    "\n",
    "# close file\n",
    "f.close()\n",
    "print(f\"Saved the results (metrics) to: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.C) Visual presentation of the training phases metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### i) Reading old results file (optional)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "read_old_results = True\n",
    "train_results_file = \"Final_Models/Multilabel_Train_Results_Final.json\"\n",
    "if read_old_results:\n",
    "    Train_results = pd.read_json(train_results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ii) Plot the loss evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.array(Train_results['training']['total_loss']), label='training')\n",
    "plt.plot(np.array(Train_results['validating']['total_loss']), label='validating')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Measured loss evolution during training')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.savefig(f\"LossEvolution_Multilabel_Training_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{str(optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### iii) Plot evolution of F1 score, precision and recall computed with macro averaging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (9,3), sharey=True)\n",
    "ax1.plot(Train_results['training']['macro/f1'])\n",
    "ax1.plot(Train_results['training']['macro/precision'])\n",
    "ax1.plot(Train_results['training']['macro/recall'])\n",
    "ax2.plot(Train_results['validating']['macro/f1'])\n",
    "ax2.plot(Train_results['validating']['macro/precision'])\n",
    "ax2.plot(Train_results['validating']['macro/recall'])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Metrics\")\n",
    "ax1.set_title(\"Training\")\n",
    "ax2.set_title(\"Validation\")\n",
    "ax1.legend(['F1 score', 'Precision score', 'Recall score'], fontsize=11)\n",
    "plt.suptitle('Macro averaged metrics')\n",
    "plt.savefig(f\"MetricsEvol_SingleModelTraining_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{criterion}_{str(optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### iv) Plot the hamming loss evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.array(Train_results['training']['hamming_loss']), label='Training')\n",
    "plt.plot(np.array(Train_results['validating']['hamming_loss']), label='Validation')\n",
    "plt.legend()\n",
    "plt.title('Average Hamming distance')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Hamming loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.A) Loading trained models (optional)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_model = True\n",
    "\n",
    "model_path = \"Final_Models/MultilabelModel_Final.pth\"\n",
    "if load_model:\n",
    "    if device==\"cpu\":\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Model loaded from: \\n {model_path}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.B) Running the testing and computing metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "testing_result = testing_model(test_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii) Calculating the different metrics: F1, precision and recall scores\n",
    "Function from sklearn module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report = classification_report(y_true=testing_result['ground_truth'], y_pred=testing_result['predicted'], output_dict=True, target_names=tags, zero_division=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.C) Visual presentation of the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### i) A focus on the metrics for each classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(report).iloc[:-1, :-4].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Classes average evaluation of the multi-label classifier\")\n",
    "plt.savefig(\"Multilabel_evaluation_class_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii) Different averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(report).iloc[:-1, -4:].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Average evaluation of the multilabel classifier\")\n",
    "plt.savefig(\"Multilabel_evaluation_average_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii) Additional visualisation on the distribution of each class in the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report).iloc[-1:, :-4].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title('The number of actual occurrences of the class in the testing dataset')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
