{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planet : Amazon space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_folder = '../IPEO_Planet_project'\n",
    "if not os.path.exists(data_folder):\n",
    "    data_folder = input(\"Enter the data folder path: \")\n",
    "    assert os.path.exists(data_folder), \"I did not find the folder at, \"+str(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "### Understanding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels_dt = pd.read_csv(f'{data_folder}/train_labels.csv',dtype=str)\n",
    "tags = pd.Series(labels_dt['tags'].str.split(expand=True).stack().unique())\n",
    "#print(F\"{len(tags)} unique tags are: \\n{tags.tolist()} \")\n",
    "shape1 = labels_dt.shape\n",
    "for tag in tags:\n",
    "     a = [(tag in i.split()) for i in labels_dt['tags']]\n",
    "     new_col = np.zeros(len(a),dtype=int)+a\n",
    "     #print(new_col.shape)\n",
    "     labels_dt[tag]=new_col\n",
    "print(f\" \\n Shape of the initial dataset {shape1} and final {labels_dt.shape}\")\n",
    "labels_dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making sure the dataset is clean:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def checking_folder(data_folder = data_folder):\n",
    "    corrupted_files = []\n",
    "    i = 0\n",
    "    for img_id in tqdm(labels_dt['image_name']):\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(f\"image {i}\")\n",
    "        img_name = os.path.join(f'{data_folder}/train-jpg', f'{img_id}.jpg')\n",
    "        img = imread(img_name)\n",
    "        if img is None:\n",
    "            print(f'{img_id} is corrupt...')\n",
    "            corrupted_files.append(img_id)\n",
    "        img = None\n",
    "        i = i + 1\n",
    "    return corrupted_files\n",
    "\n",
    "#checking_folder()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom made Class for multilabel classification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this tutorial for help:\n",
    "<https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AmazonSpaces(Dataset):\n",
    "    \"\"\"Amazon aerial image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "\n",
    "        self.tags = self.labels['tags'].str.split(expand=True).stack().unique()\n",
    "        for tag in self.tags:\n",
    "            a = [(tag in i.split()) for i in self.labels['tags']]\n",
    "            self.labels[tag]=np.zeros(len(a),dtype=int)+a\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.labels.iloc[idx, 0])\n",
    "        img_name = f'{img_name}.jpg'\n",
    "        image = imread(img_name)\n",
    "        #image = torch.from_numpy(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = self.labels.loc[idx, self.tags].to_numpy(dtype=np.float64)\n",
    "        #labels = torch.from_numpy(labels)\n",
    "        #other output\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        #print(type(image), type(labels))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Amazon_dataset = AmazonSpaces(csv_file=f'{data_folder}/train_labels.csv',\n",
    "                                    root_dir=f'{data_folder}/train-jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having some fun with transforms\n",
    "Just testing some transforms and how to implement into dataset\n",
    "First creating and Adjust_Saturation class to be able to apply adjust_saturation the same way as the other transforms class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AdjustSaturation(object):\n",
    "    \"\"\"Adjust the saturation of a tensor image.\n",
    "    Args:\n",
    "        saturation factor (float): if 0 -> black and white, if 1 -> same as the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, saturation_factor):\n",
    "        assert isinstance(saturation_factor, (int,float))\n",
    "        self.saturation_factor = saturation_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        new_tensor = transforms.functional.adjust_saturation(img, self.saturation_factor)\n",
    "\n",
    "        return new_tensor\n",
    "\n",
    "Test_transforms = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256),transforms.RandomAutocontrast(p=1), AdjustSaturation(1.2)])\n",
    "\n",
    "transformed_dataset = AmazonSpaces(csv_file=f'{data_folder}/train_labels.csv',\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "N=3\n",
    "for i in range(N+1):\n",
    "    #Image from original direct dataset\n",
    "    #image, labels = Amazon_dataset[i]\n",
    "    sample = Amazon_dataset[i]\n",
    "    labels = sample['labels']\n",
    "    image = sample['image']\n",
    "\n",
    "\n",
    "    ax = plt.subplot(2, N+1, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(f' {tags[labels==1]}')\n",
    "    ax.axis('off')\n",
    "    img = F.to_pil_image(image)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    #Images from transformed_dataset\n",
    "\n",
    "    sample_tf = transformed_dataset[i]\n",
    "    labels_tf = sample_tf['labels']\n",
    "    image_tf = sample_tf['image']\n",
    "\n",
    "    ax = plt.subplot(2, N+1, i + 5)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(f' {tags[labels_tf==1]}')\n",
    "    ax.axis('off')\n",
    "    img = F.to_pil_image(image_tf)\n",
    "    ax.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(transformed_dataset, batch_size=64,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "def show_4_image_in_batch(sample_batched, tags = tags):\n",
    "    images_batch, labels = sample_batched['image'], sample_batched['labels'] #old class output\n",
    "    #images_batch, labels = sample_batched\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4)\n",
    "    for i in range(4):\n",
    "        img = F.to_pil_image(images_batch[i])\n",
    "        axs[i].imshow(img)\n",
    "        ids = labels[i,:].numpy()\n",
    "        axs[i].set_title(f'#{i}:\\n {tags[ids == 1]}')\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(f\"Batch # {i_batch} with sizes: {sample_batched['image'].size(), sample_batched['labels'].size()}\")\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        show_4_image_in_batch(sample_batched)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer convulotional network\n",
    "## Model\n",
    "Function of interest:\n",
    "* torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "* torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "* torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size = 5) #Input is a 3 plane 256x256 tensor (RBB) -> output 10 planes of 254x254\n",
    "        self.pool_max = nn.MaxPool2d(2, 2) #output of dim-2 x dim-2\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5) #input 10 planes 127x127, output 20 planes of 125x125\n",
    "        self.pool_avg = nn.AvgPool2d(4,4)\n",
    "        self.fc = nn.Linear(20*27*27,17) #single dense layer for the network\n",
    "        self.batchNorm = nn.BatchNorm1d(256)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        #print(f\"step 1 : {x.shape}\")\n",
    "        x = self.pool_max(nn.functional.relu(self.conv1(x)))\n",
    "        #print(f\"step 2 : {x.shape}\")\n",
    "        x = self.pool_avg(x)\n",
    "        #print(f\"step 3 : {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        #print(f\"step 4 : {x.shape}\")\n",
    "        x = x.view(-1, 20*27*27)\n",
    "        #print(f\"step 5 : {x.shape}\")\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiLayerCNN()\n",
    "summary(net,input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training, Validation and accuracy functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate(net, dataloader,loss_fn=nn.BCELoss()):\n",
    "    net.eval()\n",
    "    count,acc,loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in dataloader:\n",
    "            out = net(features)\n",
    "            loss += loss_fn(out,labels)\n",
    "            pred = torch.max(out,1)[1]\n",
    "            acc += (pred==labels).sum()\n",
    "            count += len(labels)\n",
    "    return loss.item()/count, acc.item()/count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = nn.BCELoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count = 0,0,0\n",
    "    for features,labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return total_loss.item()/count, acc.item()/count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(net,train_loader,test_loader,optimizer=None,lr=0.01,epochs=2,loss_fn=nn.BCELoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    res = { 'train_loss' : [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for ep in range(epochs):\n",
    "        tl,ta = train_epoch(net,train_loader,optimizer=optimizer,lr=lr,loss_fn=loss_fn)\n",
    "        vl,va = validate(net,test_loader,loss_fn=loss_fn)\n",
    "        print(f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\")\n",
    "        res['train_loss'].append(tl)\n",
    "        res['train_acc'].append(ta)\n",
    "        res['val_loss'].append(vl)\n",
    "        res['val_acc'].append(va)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Full model running\n",
    "Ideas from :\n",
    "<https://learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/>\n",
    "## 1) Putting the model on the gpu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MultiLayerCNN().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Getting the different datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_csv = data_folder+'/train.csv'\n",
    "validation_csv = data_folder+'/validation.csv'\n",
    "test_csv = data_folder+'/test.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Transform_choice = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256),transforms.RandomAutocontrast(p=1), AdjustSaturation(1.2)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = AmazonSpaces(csv_file=train_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "validation_dataset = AmazonSpaces(csv_file=validation_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "test_dataset = AmazonSpaces(csv_file=test_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Wrapping into the different dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Choice of Criterion and Opitimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/model_classification.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
