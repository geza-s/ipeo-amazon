{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0692f2-9b4e-43aa-bc7f-49af92e9767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torchvision.transforms.functional\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Multi_Model_Amazon_Engine import train_dual, batch_prediction_dual\n",
    "from Multi_Model_Amazon_Module import AmazonSpaceDual, GroundCNN, CloudCNN, AdjustSaturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3c39a1-5bb0-4180-bd96-df37d25c26f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the data folder path:  /Users/annahalloy/Documents/EPFL/MA1/IPEO/dataset_amazon\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../IPEO_Planet_project'\n",
    "if not os.path.exists(data_folder):\n",
    "    data_folder = input(\"Enter the data folder path: \")\n",
    "    assert os.path.exists(data_folder), \"I did not find the folder at, \"+str(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc102319-3829-4221-8f10-92ddc030ea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dfcce3-a485-4eb2-a2e5-232a728e17f6",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73d736-fc21-4cb6-81c4-bd158acabc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'training.csv'\n",
    "validation_csv = 'validation.csv'\n",
    "test_csv = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f716f87c-8f8a-46c7-89af-c2032722d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AmazonSpaceDual(csv_file=train_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "validation_dataset = AmazonSpaceDual(csv_file=validation_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "test_dataset = AmazonSpaceDual(csv_file=test_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f95528-7a87-4698-9ec0-a619bf35364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "## wrap into different dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size,shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0437d5c3-33f6-404c-9f2b-f4d7789f5e96",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of DataLoader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/068gq44s69d9lyrh82smzd480000gn/T/ipykernel_1516/2496841431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of DataLoader"
     ]
    }
   ],
   "source": [
    "#torch.tensor(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93192257-7e99-48f4-acc3-c95cf891845f",
   "metadata": {},
   "source": [
    "### Define different transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3249a19-3c19-4225-b1a7-f4a56f19777f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/068gq44s69d9lyrh82smzd480000gn/T/ipykernel_1516/134276372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Transform_choice = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "#Transform_choice = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256)])\n",
    "#mean = \n",
    "\n",
    "#normalize = T.Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "372df457-d5c6-41a2-a311-a5e756008996",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = T.Compose([  \n",
    "  T.RandomGrayscale(),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.RandomApply([T.GaussianBlur(kernel_size=7)]),\n",
    "  T.RandomPosterize(bits=8),\n",
    "  T.RandomVerticalFlip(),\n",
    "  T.ColorJitter(),\n",
    "    \n",
    "  T.Resize((256, 256)), #multi_model takes input size 256x256\n",
    "  T.ToTensor(),\n",
    "  #normalize\n",
    "])\n",
    "\n",
    "# we do not augment the validation dataset (aside from resizing and tensor casting)\n",
    "transforms_val = T.Compose([\n",
    "  T.Resize((256, 256)), \n",
    "  T.ToTensor(),\n",
    "  #normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7480b1d6-f87c-45f5-b8ce-eef932c9d301",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/068gq44s69d9lyrh82smzd480000gn/T/ipykernel_1516/2566317233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the class proportions in the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Get the class proportions in the training set\n",
    "label_counts = {}\n",
    "for data, labels in train_dataset:\n",
    "    for label in labels:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "\n",
    "# Calculate the class weights\n",
    "label_weights = {}\n",
    "max_count = max(label_counts.values())\n",
    "for label, count in label_counts.items():\n",
    "    label_weights[label] = max_count / count\n",
    "\n",
    "# Define a custom sampler that applies the transform and class weights\n",
    "class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset, transform, weights):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.weights = weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Iterate over the indices of the dataset\n",
    "        for idx in range(len(self.dataset)):\n",
    "            # Get the data and label at the current index\n",
    "            data, label = self.dataset[idx]\n",
    "\n",
    "            # Apply the transform to the data\n",
    "            if self.transform is not None:\n",
    "                data = self.transform(data)\n",
    "\n",
    "            # Return the data and label, weighted by the class weights\n",
    "            yield data, label, self.weights[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Create an instance of the BalancedSampler\n",
    "sampler = BalancedSampler(train_set, transform, label_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
