{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planet : Amazon space -- Full model running for multilabel classification\n",
    "This file is the main processing file for the multilabel classification by a single CNN model.\n",
    "Important information:\n",
    "- The part 0 to 4 are necessary for the training AND the testing.\n",
    "- The training part can be skipped to simply test the model: simply skip the whole part 5.\n",
    "- Donâ€™t forget to load a trained model or results file if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Initialization\n",
    "## A) Getting Module and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from MultiModel_Amazon_Engine import *\n",
    "from MultiModel_Amazon_Module import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Folder with images and labels csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_folder = '../IPEO_Planet_project'\n",
    "if not os.path.exists(data_folder):\n",
    "    data_folder = input(\"Enter the data folder path: \")\n",
    "    assert os.path.exists(data_folder), \"I did not find the folder at, \"+str(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Creating model and transferring to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ground_model = GroundCNN().to(device)\n",
    "cloud_model = CloudCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "summary(ground_model, input_size=(48,3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "summary(cloud_model, input_size=(48,3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Loading the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_csv = 'training.csv'\n",
    "validation_csv = 'validation.csv'\n",
    "test_csv = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Transform_choice = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(256),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = AmazonSpaceDual(csv_file=train_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "validation_dataset = AmazonSpaceDual(csv_file=validation_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)\n",
    "test_dataset = AmazonSpaceDual(csv_file=test_csv,\n",
    "                                    root_dir=f'{data_folder}/train-jpg', transform=Transform_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Wrapping into the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "# Slow but stable training:\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "# For a quicker training, num_workers to 6:\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size,  num_workers=6, shuffle=True, drop_last = True)\n",
    "#validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=6, shuffle=True, drop_last = True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Choice of Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "ground_crit = nn.BCEWithLogitsLoss()\n",
    "cloud_crit = nn.BCELoss()\n",
    "\n",
    "ground_optim = torch.optim.SGD(ground_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "cloud_optim = torch.optim.SGD(cloud_model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5) TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Training the cloud labels classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs_number = 2\n",
    "\n",
    "Cloud_results = train_solo(\"cloud_model\",cloud_model, train_dataloader, validation_dataloader, device=device,optimizer=cloud_optim, lr = learning_rate, epochs=epochs_number, loss_fn=cloud_crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A-i) Saving cloud model training results into a Json file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create json object from dictionary\n",
    "js = json.dumps(Cloud_results)\n",
    "# open file for writing, \"w\"\n",
    "name = f\"CloudModel_training_results_{epochs_number}epochs_{batch_size}batchsize_{learning_rate}lr_{cloud_crit}_crit_{str(cloud_optim).split()[0]}_optim_{date.today()}.json\"\n",
    "f = open(name, \"a\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(js)\n",
    "print(f\"Saved the results (metrics) to: {name}\")\n",
    "\n",
    "# close file\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A-ii) Loading old results file (optional)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_cloud_json = False\n",
    "\n",
    "cloud_results_file = \"Final_Models/CloudModel_Train_Results_Final.json\"\n",
    "\n",
    "if load_cloud_json:\n",
    "    json_file = open(cloud_results_file)\n",
    "    if json_file:\n",
    "        print(f\"Loading results from json file from {cloud_results_file}\")\n",
    "        Cloud_results = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A-iii) Visualisation of the training and validation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A) Plot the loss evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "x = range(1,len(Cloud_results['training']['total_loss'])+1)\n",
    "plt.plot(x, np.array(Cloud_results['training']['total_loss']), label = 'training')\n",
    "plt.plot(x, np.array(Cloud_results['validating']['total_loss']), label = 'validation')\n",
    "#plt.title('Training: measured loss evolution')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.xticks(x)\n",
    "plt.savefig(f\"LossEvolution_CloudModelTraining_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{cloud_crit}_{str(cloud_optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### B) Plot evolution of F1 score, precision and recall computed with macro averaging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (9,3), sharey=True)\n",
    "ax1.plot(Cloud_results['training']['macro/f1'])\n",
    "ax1.plot(Cloud_results['training']['macro/precision'])\n",
    "ax1.plot(Cloud_results['training']['macro/recall'])\n",
    "ax2.plot(Cloud_results['validating']['macro/f1'])\n",
    "ax2.plot(Cloud_results['validating']['macro/precision'])\n",
    "ax2.plot(Cloud_results['validating']['macro/recall'])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Metrics\")\n",
    "ax1.set_title(\"Training\")\n",
    "ax2.set_title(\"Validation\")\n",
    "ax2.legend(['F1 score', 'Precision score', 'Recall score'], fontsize=11)\n",
    "plt.suptitle(\"Cloud label's model training\\n\")\n",
    "plt.savefig(f\"Metrics_CloudModelTraining_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{cloud_crit}_{str(cloud_optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.savefig(f\"CloudModel_Training_Metrics_evolution.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## B) Training the ground labels classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs_number = 2\n",
    "\n",
    "Ground_results = train_solo(\"ground_model\",ground_model, train_dataloader, validation_dataloader, device=device,optimizer=ground_optim, lr = learning_rate, epochs=epochs_number, loss_fn=ground_crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B-i) Saving ground model training results into a Json file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create json object from dictionary\n",
    "js = json.dumps(Ground_results)\n",
    "\n",
    "# open file for writing, \"w\"\n",
    "name = f\"GroundModel_training_results_{epochs_number}epochs_{batch_size}batchsize_{learning_rate}lr_{ground_crit}_crit_{str(ground_optim).split()[0]}_optim_{date.today()}.json\"\n",
    "f = open(name, \"a\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(js)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-ii) Loading old results file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "load_ground_json = False\n",
    "\n",
    "ground_results_file = \"Final_Models/GroundModel_Train_Results_Final.json\"\n",
    "\n",
    "if load_ground_json:\n",
    "    json_file = open(ground_results_file)\n",
    "    if json_file:\n",
    "        print(f\"Loading results from json file from {ground_results_file}\")\n",
    "        Ground_results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B-iii) Visualisation of the training and validation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A) Plot the loss evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.array(Ground_results['training']['total_loss']), label = 'training')\n",
    "plt.plot(np.array(Ground_results['validating']['total_loss']), label = 'validation')\n",
    "plt.title('Measured loss evolution during training', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel(\"Binary cross entropy loss\")\n",
    "plt.savefig(f\"LossEvolution_GroundModelTraining_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{cloud_crit}_{str(cloud_optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.savefig(\"GroundModel_Training_Loss.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### B) Plot evolution of F1 score, precision and recall computed with macro averaging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (9,3), sharey=True)\n",
    "ax1.plot(Ground_results['training']['macro/f1'])\n",
    "ax1.plot(Ground_results['training']['macro/precision'])\n",
    "ax1.plot(Ground_results['training']['macro/recall'])\n",
    "ax2.plot(Ground_results['validating']['macro/f1'])\n",
    "ax2.plot(Ground_results['validating']['macro/precision'])\n",
    "ax2.plot(Ground_results['validating']['macro/recall'])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Metrics\")\n",
    "ax1.set_title(\"Training\")\n",
    "ax2.set_title(\"Validation\")\n",
    "ax1.legend(['F1 score', 'Precision score', 'Recall score'], fontsize=11)\n",
    "plt.suptitle(\"Ground label's model training\\n\")\n",
    "plt.savefig(f\"Metrics_GroundModelTraining_{batch_size}batchsize_{str(learning_rate).replace('.','_')}lr_{cloud_crit}_{str(cloud_optim).split()[0]}_optim_{date.today()}.png\")\n",
    "plt.savefig(f\"GroundModel_Training_Metrics_evolution.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### C) Plot the hamming loss evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.array(Ground_results['training']['hamming_loss']), label='Training')\n",
    "plt.plot(np.array(Ground_results['validating']['hamming_loss']), label='Validation')\n",
    "plt.legend()\n",
    "plt.title('Average Hamming distance')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Hamming loss\")\n",
    "plt.savefig(\"GroundModel_Training_Hamming_Loss.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A) Load previous models (if necessary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_models = True\n",
    "\n",
    "ground_model_path = 'Final_Models/GroundModel_Final.pth'\n",
    "cloud_model_path = 'Final_Models/CloudModel_Final.pth'\n",
    "\n",
    "if load_models:\n",
    "    print(\"Loading old models...\")\n",
    "    if not os.path.exists(ground_model_path):\n",
    "        print('Did not find ground pretrained model file')\n",
    "    if not os.path.exists(cloud_model_path):\n",
    "        print('Did not find cloud pretrained model file')\n",
    "    else:\n",
    "        if device==\"cpu\":\n",
    "            ground_model.load_state_dict(torch.load(f'groundmodel_NewClassifier_{date.today()}.pth', map_location=torch.device('cpu')))\n",
    "            cloud_model.load_state_dict(torch.load(f'cloudmodel_NewClassifier_{date.today()}.pth', map_location=torch.device('cpu')))\n",
    "        else:\n",
    "            ground_model.load_state_dict(torch.load(ground_model_path))\n",
    "            ground_model.to(device)\n",
    "            cloud_model.load_state_dict(torch.load(cloud_model_path))\n",
    "            cloud_model.to(device)\n",
    "    print(\".... loaded !\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_results = testing_multi(test_dataloader, ground_model, cloud_model, device = device,\n",
    "                                criterion_gr=nn.BCEWithLogitsLoss(), criterion_cl =nn.BCELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## B) Testing the ground label's model\n",
    "### i) Calculating the different metrics: F1, precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_ground = classification_report(y_true=testing_results['ground']['target'], \n",
    "                                      y_pred=testing_results['ground']['predicted'],\n",
    "                                      target_names = test_dataloader.dataset.tags_ground.keys(),\n",
    "                                      output_dict=True, zero_division=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii) Visualisation: Classes averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_ground).iloc[:-1, :-4].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Classes average evaluation of the ground label's classifier\")\n",
    "plt.savefig(\"GroundModel_evaluation_class_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii) Visualisation: Different averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_ground).iloc[:-1, -4:].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Average evaluation of the ground label's classifier\")\n",
    "plt.savefig(\"GroundModel_evaluation_average_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iv) Additional visualisation on the distribution of the labels in the testing dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_ground).iloc[-1:, :].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title('The number of actual occurrences of the class in the testing dataset')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Testing the cloud label's model\n",
    "### i) Calculating the different metrics: F1, precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cloud = classification_report(y_true=testing_results['cloud']['target'], \n",
    "                                      y_pred=testing_results['cloud']['predicted'],\n",
    "                                      target_names = test_dataloader.dataset.tags_cloud.keys(),\n",
    "                                      output_dict=True, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii) Visualisation: Classes averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_cloud).iloc[:-1, :-4].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Classes average evaluation of the cloud label's classifier\")\n",
    "plt.savefig(\"CloudModel_evaluation_class_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii) Visualisation: Different averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_cloud).iloc[:-1, -4:].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title(\"Average evaluation of the cloud label's classifier\")\n",
    "plt.savefig(\"CloudModel_evaluation_average_results.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iv) Additional visualisation on the distribution of the labels in the testing dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_cloud).iloc[-1:, :].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title('The number of actual occurrences of the class in the testing dataset')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) The combined 2 previous classifiers\n",
    "### i) Calculating the different metrics: F1, precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = list(test_dataloader.dataset.tags_ground.keys()) + list(test_dataloader.dataset.tags_cloud.keys())\n",
    "report_total = classification_report(y_true=testing_results['total']['target'], \n",
    "                                      y_pred=testing_results['total']['predicted'],\n",
    "                                      target_names = all_tags,\n",
    "                                      output_dict=True, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii) Visualisation: Classes averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_total).iloc[:-1, :-4].T, annot=True, cmap=\"PiYG\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii) Visualisation: Different averaged metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_total).iloc[:-1, -4:].T, annot=True, cmap=\"PiYG\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iv) Additional visualisation on the distribution of the labels in the testing dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.DataFrame(report_total).iloc[-1:, :].T, annot=True, cmap=\"PiYG\")\n",
    "plt.title('The number of actual occurrences of the class in the testing dataset')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
